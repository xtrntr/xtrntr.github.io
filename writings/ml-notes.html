<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="css/style.css" rel="stylesheet" type="text/css">
        <title>Notes on Machine Learning</title>
    </head>

    <body>
        <main>
            <div id="text">
                <h2>A review of Andrew Ng's machine learning course</h1>
                <p>
                    This course is a gentle introduction to machine learning as the main idea in many of the lessons is to impart the intuition of how different machine learning techniques work. There is some math, but the notation shouldn't scare off anybody, personally writing down the math equations helped with getting the idea to stick with me rather than passively consuming the information. Programming-wise it's not too challenging as well, although it may be a little frustrating for a complete newbie to get started with MATLAB/Octave. Psuedocode comes up in the lectures often, but it's mostly algorithms that iterate with for-loops (gradient descent, backpropagation). The hardest part is possibly the mapping the mathematical formulas to code and wrapping one's head around vectorized code.
                </p>

                <p>
                    The first part of the course covers supervised learning methods under Linear Regression, Logistic Regression, simple supervised Neural Networks and Linear/Gaussian Kernel Support Vector Machines (large-margin classifiers). These techniques are covered briefly and again, the main thing is to impart the intuition of how the maths behind these methods work. Some more in-depth examples of what's covered are:
                    <ul>
                        <li>applications of ML models (classification, prediction)</li>
                        <li>debugging common errors (i.e. gradient descent is not converging of wrong learning rate alpha)</li>
                        <li>selecting and adding features (linear, polynomial).</li>
                        <li>overfitting & underfitting, applying regularization, feature scaling</li>
                        <li>when to get more data vs fixing a faulty model.</li>
                        <li>misunderstanding the purpose of training, cross-validation and test sets.</li>
                    </ul>
                </p>

                <p>
                    The second part of the course covers unsupervised learning/dimensionality reduction methods like K-Means, Principal Component Analysis the motivation for doing so (visualization, remove redundant features, software engineering concerns)
                </p>

                <!-- <h2>Some possibly interesting notes</h2> -->
                <!-- <h2>Week 9 - Anomaly Detection vs Supervised learning</h2> -->

            </div>
        </main>
    </body>
</html>
